{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55e7f86-2eb4-429e-97f8-d410479e712e",
   "metadata": {},
   "source": [
    "# Hugging Face - Question Answering\n",
    "\n",
    "Extractive question-answering system: Model extracts a part of the given reference (reference or context: A paragraph or sentence in which you want to find the answer to your question) to answer a question (question: The question of which answer you want from the model)\n",
    "\n",
    "Abstractive or Generative question-answering system: Model generates some new words or sentences to answer from the context that correctly answers the question.\n",
    "\n",
    "We will be building an extractive question-answering system that only needs a Bert or Transformer encoder-only architecture unlike a Generative question answering system which will require a whole transformer ( encoder+decoder) architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae403529-5fe5-406b-aa75-719100ec0ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import DefaultDataCollator\n",
    "import datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7affb21-2e10-40af-adfe-c2083c482033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (C:/Users/jorda/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 158.82it/s]\n"
     ]
    }
   ],
   "source": [
    "squad = load_dataset(\"squad\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18edccf4-c546-4318-a9e7-554d1df6be80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad.get(\"train\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f74d97-beca-42e9-8ee2-4aa584d1eaa8",
   "metadata": {},
   "source": [
    "Id- Id of that question\r\n",
    "\r\n",
    "Title- The topic, question belongs to\r\n",
    "\r\n",
    "Context- The context in which the model has to find an answer to the question\r\n",
    "\r\n",
    "Question- The question itself\r\n",
    "\r\n",
    "Answer- Part of context which can be the answer of the question & index where the answer starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0de7d40-6760-4449-bda7-056d5aa50503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='deepset/bert-base-cased-squad2', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c3535-2b14-4d22-ac0f-842d44f1dfd4",
   "metadata": {},
   "source": [
    "Now let’s define a “Preprocess” function, which tokenizes the whole data, does some cleaning, and preprocessing of text, and performs required changes in order to convert the data into a form that we can feed the model\n",
    "\n",
    "**NOTE** - There are things happening in this function I do not fully understand, treating as a magic function for now. It essentially adds the start and end positions for a given context s answer location as character index locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07f406b8-4848-429e-a359-668a9d0ca5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=512,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7943765a-c0bf-4d00-beac-d836d5093b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\jorda\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-a7926d51eaae8d08.arrow\n",
      "Loading cached processed dataset at C:\\Users\\jorda\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-cc816901e4102791.arrow\n"
     ]
    }
   ],
   "source": [
    "# In batches or SQUAD dataset items apply the preprocess function.\n",
    "tokenized_squad = squad.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab88518-8c36-49dd-97ce-64a7a1c1eea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n",
      "142\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_squad['train']['start_positions'][0])\n",
    "print(tokenized_squad['train']['end_positions'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d97658-f151-4128-89e9-061834a3c3b2",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "# Get a pretrained BERT model, which used SQUAD2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e85b7a78-5668-4f24-8bae-222434db6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator()\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53d3902b-df47-4361-8bd5-9a0997afeff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c322de7-4548-476e-8f8c-452eac743436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 06:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.829199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.478194465637207, metrics={'train_runtime': 385.7592, 'train_samples_per_second': 0.259, 'train_steps_per_second': 0.13, 'total_flos': 26129675673600.0, 'train_loss': 0.478194465637207, 'epoch': 1.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = datasets.Dataset.from_pandas(pd.DataFrame(data=tokenized_squad['train'][:100]))\n",
    "testing_set = datasets.Dataset.from_pandas(pd.DataFrame(data=tokenized_squad['validation'][:20]))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    " output_dir=\"./results\",\n",
    " evaluation_strategy=\"epoch\",\n",
    " learning_rate=2e-5,\n",
    " per_device_train_batch_size=2,\n",
    " per_device_eval_batch_size=2,\n",
    " num_train_epochs=1,\n",
    " weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    " model=model,\n",
    " args=training_args,\n",
    " train_dataset=training_set,\n",
    " eval_dataset=testing_set,\n",
    " tokenizer=tokenizer,\n",
    " data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17228df7-bd6b-4e45-adc7-e33fd6858d1a",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20be77b9-b269-4c3f-a535-745e7acd5475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91e600ed-818d-41a7-87c0-521c3dca6e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_answer(question,text):\n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    return tokenizer.decode(predict_answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f92e04cd-aede-4a32-b19e-9887d38d7d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "question =  \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\"\n",
    "text = \"It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\"\n",
    "\n",
    "print(find_answer(question,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99d3e26c-ddd5-441b-a5d3-432a11015528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brittany\n"
     ]
    }
   ],
   "source": [
    "question =  \"What is the first major city west of paris?\"\n",
    "text = \"Marseille is too the south, nannes to the east, brittany to the west\"\n",
    "\n",
    "print(find_answer(question,text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
